---
title: Constrained fits to data
date: 2013/06/11 19:39:59
updated: 2013/06/12 08:31:16
categories: data analysis, optimization
tags: 
---


<p>
Our objective here is to fit a quadratic function in the least squares sense to some data, but we want to constrain the fit so that the function has specific values at the end-points. The application is to fit a function to the lattice constant of an alloy at different compositions. We constrain the fit because we know the lattice constant of the pure metals, which are at the end-points of the fit and we want these to be correct. 
</p>

<p>
We define the alloy composition in terms of the mole fraction of one species, e.g. \(A_xB_{1-x}\). For \(x=0\), the alloy is pure B, whereas for \(x=1\) the alloy is pure A. According to Vegard's law the lattice constant is a linear composition weighted average of the pure component lattice constants, but sometimes small deviations are observed. Here we will fit a quadratic function that is constrained to give the pure metal component lattice constants at the end points. 
</p>

<p>
The quadratic function is \(y = a x^2 + b x + c\). One constraint is at \(x=0\) where \(y = c\), or \(c\) is the lattice constant of pure B. The second constraint is at \(x=1\), where \(a + b + c\) is equal to the lattice constant of pure A. Thus, there is only one degree of freedom. \(c = LC_B\), and \(b = LC_A - c - a\), so \(a\) is our only variable.
</p>

<p>
We will solve this problem by minimizing the summed squared error between the fit and the data. We use the <code>fmin</code> function in <code>scipy.optimize</code>. First we create a fit function that encodes the constraints. Then we create an objective function that will be minimized. We have to make a guess about the value of \(a\) that minimizes the summed squared error. A line fits the data moderately well, so we guess a small value, i.e. near zero, for \(a\). Here is the solution.
</p>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #8b0000;">import</span> numpy <span style="color: #8b0000;">as</span> np
<span style="color: #8b0000;">import</span> matplotlib.pyplot <span style="color: #8b0000;">as</span> plt

<span style="color: #ff0000; font-weight: bold;"># </span><span style="color: #ff0000; font-weight: bold;">Data to fit to</span>
<span style="color: #ff0000; font-weight: bold;"># </span><span style="color: #ff0000; font-weight: bold;">x=0 is pure B</span>
<span style="color: #ff0000; font-weight: bold;"># </span><span style="color: #ff0000; font-weight: bold;">x=1 is pure A</span>
X = np.array([0.0, 0.1,  0.25, 0.5,  0.6,  0.8,  1.0])
Y = np.array([3.9, 3.89, 3.87, 3.78, 3.75, 3.69, 3.6])

<span style="color: #8b0000;">def</span> <span style="color: #8b2323;">func</span>(a, XX):
    LC_A = 3.6
    LC_B = 3.9

    c = LC_B
    b = LC_A - c - a

    yfit = a * XX**2 + b * XX + c
    <span style="color: #8b0000;">return</span> yfit

<span style="color: #8b0000;">def</span> <span style="color: #8b2323;">objective</span>(a):
    <span style="color: #228b22;">'function to minimize'</span>
    SSE = np.sum((Y - func(a, X))**2)
    <span style="color: #8b0000;">return</span> SSE


<span style="color: #8b0000;">from</span> scipy.optimize <span style="color: #8b0000;">import</span> fmin

a_fit = fmin(objective, 0)
plt.plot(X, Y, <span style="color: #228b22;">'bo '</span>)

x = np.linspace(0, 1)
plt.plot(x, func(a_fit, x))
plt.savefig(<span style="color: #228b22;">'images/constrained-quadratic-fit.png'</span>)
</pre>
</div>

<pre class="example">
Optimization terminated successfully.
         Current function value: 0.000445
         Iterations: 19
         Function evaluations: 38
</pre>

<p>
Here is the result:
<p><img src="/img/./images/constrained-quadratic-fit.png"><p>
</p>

<p>
You can see that the end points go through the end-points as prescribed. 
</p>
<p>Copyright (C) 2013 by John Kitchin. See the <a href="/copying.html">License</a> for information about copying.<p><p><a href="/org/2013/06/11/Constrained-fits-to-data.org">org-mode source</a><p>